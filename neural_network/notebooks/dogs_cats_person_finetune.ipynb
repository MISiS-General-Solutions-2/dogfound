{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18609e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.9.1+cu102 (GeForce RTX 2060)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "clear_output()\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94285444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5934.5625 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "mult = 1024 * 1024\n",
    "print(t / mult, r / mult, a / mult, f / mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402cf16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Dataset labels remapping for COCO 80 classes\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## Dataset labels remapping for COCO 80 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14010bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['bird', 'cat', 'dog', 'person']\n",
    "new_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "        'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679f5c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': '14', '1': '15', '2': '16', '3': '0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remap_dict = dict([(str(i), str(new_names.index(label))) for i, label in enumerate(old_names)])\n",
    "remap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a24d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels_location = '../datasets/hack_coco_finetune/valid/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107306c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/hack_coco_finetune/valid/labels/6255_734_jpg.rf.1253ef88fb7007039bfc68bc41126ed7.txt',\n",
       " '../datasets/hack_coco_finetune/valid/labels/8569_2492_jpg.rf.11a478189ac0241e10a2af35aadaa8f3.txt',\n",
       " '../datasets/hack_coco_finetune/valid/labels/6641_2361_jpg.rf.7450323e5511fa6be93cda47d38d26c3.txt',\n",
       " '../datasets/hack_coco_finetune/valid/labels/7831_183_png.rf.b2b9f0a64fb6eda7f81fa4b2ee1a99b2.txt',\n",
       " '../datasets/hack_coco_finetune/valid/labels/7650_2207_jpg.rf.090a714164eeb561f69f8856f680d0aa.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_files = os.listdir(dataset_labels_location)\n",
    "label_files = [os.path.join(dataset_labels_location, lf) for lf in label_files]\n",
    "label_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fc88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lf in label_files:\n",
    "#     fixed_lines = []\n",
    "#     with open(lf, 'r', encoding=\"utf-8\") as file:\n",
    "#         all_lines = file.readlines()\n",
    "#         for line in all_lines:\n",
    "#             splitted = line.split(' ')\n",
    "#             splitted[0] = remap_dict[splitted[0]]\n",
    "#             fixed_lines.append(\" \".join(splitted))\n",
    "#     with open(lf, 'w', encoding=\"utf-8\") as file:\n",
    "#         file.writelines(fixed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1627044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lf in label_files:\n",
    "#     fixed_lines = []\n",
    "#     with open(lf, 'r', encoding=\"utf-8\") as file:\n",
    "#         all_lines = file.readlines()\n",
    "#         for line in all_lines:\n",
    "#             splitted = line.split(' ')\n",
    "#             if splitted[0] == '14':\n",
    "#                 continue\n",
    "#             if splitted[0] == '15':\n",
    "#                 splitted[0] = '0'\n",
    "#             if splitted[0] == '16':\n",
    "#                 splitted[0] = '1'\n",
    "#             if splitted[0] == '0':\n",
    "#                 splitted[0] = '2'\n",
    "#             fixed_lines.append(\" \".join(splitted))\n",
    "#     with open(lf, 'w', encoding=\"utf-8\") as file:\n",
    "#         file.writelines(fixed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ac9444",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for lf in label_files:\n",
    "#     with open(lf, 'r', encoding=\"utf-8\") as file:\n",
    "#         print(lf, '\\n', file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf139ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5\r\n"
     ]
    }
   ],
   "source": [
    "!cd ../yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96dcdafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c8fec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Train stage\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## Train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0dc417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c88d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ yolov5l –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ yolov5l –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db5f68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5/data/hack_data.yaml, weights=['yolov5l.pt'], batch_size=32, imgsz=640, conf_thres=0.05, iou_thres=0.45, task=train, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/hack_coco_finetune/train/labels' images and labels.\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/hack_coco_finetune/train/labels.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        895       1308      0.717      0.472      0.613       0.43\n",
      "              person        895        390      0.828      0.797      0.837      0.525\n",
      "                bird        895         65      0.408      0.508      0.403      0.225\n",
      "                 cat        895        166      0.914      0.319      0.629      0.536\n",
      "                 dog        895        687      0.719      0.265      0.581      0.433\n",
      "Speed: 0.1ms pre-process, 9.8ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5l.pt --verbose --task train --data hack_data.yaml --img 640 --batch-size 32 --conf-thres 0.05 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b09e95fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –§–∞–π–Ω-—Ç—å—é–Ω–∏–Ω–≥ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –§–∞–π–Ω-—Ç—å—é–Ω–∏–Ω–≥ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "433df535",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=hack_data.yaml, hyp=hyp.finetune.yaml, epochs=11, batch_size=8, imgsz=600, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=projects/COCO-street-finetune-2, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 4 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0032, lrf=0.12, momentum=0.843, weight_decay=0.00036, warmup_epochs=2.0, warmup_momentum=0.5, warmup_bias_lr=0.05, box=0.0296, cls=0.243, cls_pw=0.631, obj=0.301, obj_pw=0.911, iou_t=0.2, anchor_t=2.91, fl_gamma=0.0, hsv_h=0.0138, hsv_s=0.664, hsv_v=0.464, degrees=0.373, translate=0.245, scale=0.898, shear=0.602, perspective=0.0, flipud=0.00856, fliplr=0.5, mosaic=1.0, mixup=0.243, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir projects/COCO-street-finetune-2', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 468 layers, 46563709 parameters, 46563709 gradients, 109.3 GFLOPs\n",
      "\n",
      "Transferred 613/613 items from yolov5l.pt\n",
      "WARNING: --img-size 600 must be multiple of max stride 32, updating to 608\n",
      "Scaled weight_decay = 0.00036\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 101 weight, 104 weight (no decay), 104 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/hack_coco_finetune/train/labels.cache' images and l\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 895/895 [00:01<00:00, 658.51it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/hack_coco_finetune/valid/labels.cache' images and lab\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:00<00:00, 255.96it/s]\u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.33, Best Possible Recall (BPR) = 0.9954\n",
      "Image sizes 608 train, 608 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mprojects/COCO-street-finetune-2/exp\u001b[0m\n",
      "Starting training for 11 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/10     4.68G   0.02405   0.01292   0.01661        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.811      0.576      0.748      0.448\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/10     4.72G   0.02298  0.008341    0.0124        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.817      0.709      0.815      0.477\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/10     4.72G   0.02178  0.004898  0.009356        20       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.799      0.764      0.844      0.488\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/10     4.72G   0.02178  0.003748  0.008175        16       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312       0.82      0.786      0.858      0.481\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/10     4.72G   0.02206   0.00349  0.007604        31       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.774      0.858      0.871      0.476\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/10     4.72G   0.02231  0.003054  0.006975        22       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.723      0.769       0.75      0.358\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/10      4.7G   0.02154  0.003116  0.006453        22       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312       0.86       0.83      0.896      0.492\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/10     4.73G   0.02166  0.003039  0.006338        24       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.864      0.804      0.884      0.472\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/10     4.73G   0.02029  0.002958  0.005534        17       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.917      0.785      0.894      0.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/10     4.73G   0.01994  0.002911   0.00578        13       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.832      0.869      0.902      0.522\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/10     4.73G   0.01976  0.002756  0.005103        38       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.863       0.84      0.903      0.535\n",
      "\n",
      "11 epochs completed in 0.160 hours.\n",
      "Optimizer stripped from projects/COCO-street-finetune-2/exp/weights/last.pt, 93.6MB\n",
      "Optimizer stripped from projects/COCO-street-finetune-2/exp/weights/best.pt, 93.6MB\n",
      "\n",
      "Validating projects/COCO-street-finetune-2/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients, 109.1 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.863       0.84      0.903      0.535\n",
      "              person        164        155      0.875      0.845       0.92      0.529\n",
      "                 dog        164        157       0.85      0.834      0.887       0.54\n",
      "Results saved to \u001b[1mprojects/COCO-street-finetune-2/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 600 --batch 8 --hyp hyp.finetune.yaml --epochs 11 --data hack_data.yaml --weights yolov5l.pt --project \"projects/COCO-street-finetune-2\" --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b52afb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–Ω—Ç—å—é–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–Ω—Ç—å—é–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9340a41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5/data/hack_data.yaml, weights=['projects/COCO-street-finetune-2/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.05, iou_thres=0.45, task=train, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients, 109.1 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/hack_coco_finetune/train/labels.cache' images and l\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        895       1308      0.892      0.762      0.865      0.602\n",
      "              person        895        390      0.899      0.794      0.884       0.58\n",
      "                bird        895         65       0.93      0.677      0.808      0.472\n",
      "                 cat        895        166       0.84      0.777      0.864      0.699\n",
      "                 dog        895        687      0.897      0.799      0.906      0.656\n",
      "Speed: 0.1ms pre-process, 10.0ms inference, 0.9ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights projects/COCO-street-finetune-2/exp/weights/best.pt --verbose --task train --data hack_data.yaml --img 640 --batch-size 32 --conf-thres 0.05 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ec299c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15411923",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=hack_data_scratch.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=21, batch_size=8, imgsz=600, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=projects/COCO-street-from-scratch-3, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 4 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir projects/COCO-street-from-scratch-3', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1     48465  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 468 layers, 46154449 parameters, 46154449 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 607/613 items from yolov5l.pt\n",
      "WARNING: --img-size 600 must be multiple of max stride 32, updating to 608\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 101 weight, 104 weight (no decay), 104 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/hack_scratch/train/labels.cache' images and labels.\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 895/895 [00:01<00:00, 523.81it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/hack_scratch/valid/labels.cache' images and labels...\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:00<00:00, 253.77it/s]\u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.57, Best Possible Recall (BPR) = 0.9992\n",
      "Image sizes 608 train, 608 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mprojects/COCO-street-from-scratch-3/exp2\u001b[0m\n",
      "Starting training for 21 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/20     4.63G    0.1017   0.02443   0.04077        14       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.134      0.243      0.104     0.0272\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/20     4.65G   0.06791   0.02455   0.02523        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.366      0.529      0.333     0.0803\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/20     4.65G   0.06446   0.01695   0.01789        16       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.602       0.58      0.613      0.225\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/20     4.65G   0.06114   0.01461   0.01521        17       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.496      0.619      0.473      0.144\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/20     4.65G   0.05887   0.01344   0.01464        14       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.432        0.6      0.419      0.141\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/20     4.65G   0.05636   0.01183    0.0118        29       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.627      0.629      0.683      0.307\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/20     4.65G   0.05508   0.01136   0.01154        15       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312       0.58      0.672      0.643      0.267\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/20     4.65G   0.05441   0.01107   0.01157        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.712      0.746      0.765      0.344\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/20     4.65G    0.0514   0.01091   0.01118        15       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.797      0.579      0.714      0.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/20     4.65G   0.05062   0.01158   0.01139        23       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.677      0.667      0.698      0.281\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/20     4.65G   0.04757   0.01125   0.01012        21       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.762      0.698      0.768      0.351\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/20     4.65G   0.04432   0.01099  0.008552        22       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.823      0.775      0.849      0.419\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/20     4.65G    0.0393   0.01079  0.009858        15       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.883      0.693      0.809      0.416\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/20     4.65G   0.03736   0.01079  0.007529        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.856      0.824      0.881      0.451\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/20     4.65G   0.03477   0.00947  0.007291        23       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.899      0.768      0.863      0.454\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/20     4.65G   0.03285  0.009636  0.007051        16       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.815      0.824      0.857      0.469\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/20     4.65G   0.03102  0.009347  0.005164        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.888      0.811      0.879      0.478\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/20     4.65G   0.02959  0.008711  0.004025        17       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.915      0.828      0.899      0.491\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/20     4.65G   0.02925  0.009022  0.004359        18       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.882      0.871      0.897       0.51\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/20     4.65G   0.02736  0.008657  0.004436        12       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312      0.872      0.872      0.901      0.516\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/20     4.65G    0.0263  0.008111  0.003566        19       608: 100%|‚ñà| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312       0.86      0.881      0.902      0.525\n",
      "\n",
      "21 epochs completed in 0.316 hours.\n",
      "Optimizer stripped from projects/COCO-street-from-scratch-3/exp2/weights/last.pt, 92.8MB\n",
      "Optimizer stripped from projects/COCO-street-from-scratch-3/exp2/weights/best.pt, 92.8MB\n",
      "\n",
      "Validating projects/COCO-street-from-scratch-3/exp2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46124433 parameters, 0 gradients, 107.8 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        164        312       0.86      0.881      0.902      0.525\n",
      "                 dog        164        157      0.881      0.898      0.922      0.559\n",
      "              person        164        155      0.839      0.865      0.882      0.491\n",
      "Results saved to \u001b[1mprojects/COCO-street-from-scratch-3/exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 600 --batch 8 --epochs 25 --data hack_data_scratch.yaml --weights yolov5l.pt --project \"projects/COCO-street-from-scratch-3\" --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "970fca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π —Å –Ω—É–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π —Å –Ω—É–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "042c9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5/data/hack_data_scratch.yaml, weights=['projects/COCO-street-from-scratch-3/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.05, iou_thres=0.45, task=train, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46124433 parameters, 0 gradients, 107.8 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/hack_scratch/train/labels.cache' images and labels.\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        895       1308      0.924      0.957      0.972       0.68\n",
      "                bird        895         65      0.875      0.967      0.963      0.568\n",
      "                 cat        895        166          1      0.969      0.989      0.767\n",
      "                 dog        895        687      0.951      0.968      0.982      0.723\n",
      "              person        895        390      0.872      0.926      0.953      0.663\n",
      "Speed: 0.1ms pre-process, 10.0ms inference, 0.9ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights \"projects/COCO-street-from-scratch-3/exp/weights/best.pt\" --verbose --task train --data hack_data_scratch.yaml --img 640 --batch-size 32 --conf-thres 0.05 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c50a2753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ COCO val –¥–∞—Ç–∞—Å–µ—Ç–µ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ COCO val –¥–∞—Ç–∞—Å–µ—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3510d5e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5/data/coco.yaml, weights=['yolov5l.pt'], batch_size=32, imgsz=640, conf_thres=0.05, iou_thres=0.45, task=val, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco/val2017' images and labels...4952 found, 48 miss\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/coco/val2017.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.754      0.605      0.683      0.508\n",
      "              person       5000      10777      0.841      0.743      0.837      0.623\n",
      "             bicycle       5000        314      0.781      0.567      0.674      0.426\n",
      "                 car       5000       1918      0.776      0.665      0.747      0.522\n",
      "          motorcycle       5000        367      0.843      0.695      0.791      0.528\n",
      "            airplane       5000        143       0.88      0.869      0.923      0.766\n",
      "                 bus       5000        283      0.875       0.77      0.882      0.763\n",
      "               train       5000        190       0.91      0.874      0.926      0.752\n",
      "               truck       5000        414      0.671      0.531      0.639      0.455\n",
      "                boat       5000        424      0.732      0.486      0.608      0.347\n",
      "       traffic light       5000        634      0.752      0.557      0.633      0.349\n",
      "        fire hydrant       5000        101      0.939      0.812      0.918      0.785\n",
      "           stop sign       5000         75      0.834      0.733      0.803      0.741\n",
      "       parking meter       5000         60      0.756       0.62      0.692      0.538\n",
      "               bench       5000        411      0.665      0.382      0.471      0.339\n",
      "                bird       5000        427       0.76      0.492       0.62       0.43\n",
      "                 cat       5000        202      0.881      0.861      0.913      0.754\n",
      "                 dog       5000        218      0.788      0.751      0.818       0.71\n",
      "               horse       5000        272      0.843       0.82      0.887      0.702\n",
      "               sheep       5000        354      0.758      0.768      0.825      0.627\n",
      "                 cow       5000        372      0.835      0.772      0.844      0.659\n",
      "            elephant       5000        252      0.827      0.869      0.874      0.705\n",
      "                bear       5000         71      0.904      0.859      0.929      0.817\n",
      "               zebra       5000        266      0.911      0.853      0.931      0.776\n",
      "             giraffe       5000        232      0.912      0.889      0.933      0.799\n",
      "            backpack       5000        371       0.59      0.318      0.372      0.214\n",
      "            umbrella       5000        407      0.743      0.631       0.71      0.493\n",
      "             handbag       5000        540      0.606      0.308      0.378      0.224\n",
      "                 tie       5000        252      0.808       0.55      0.658      0.429\n",
      "            suitcase       5000        299      0.732      0.602      0.724      0.512\n",
      "             frisbee       5000        115      0.846       0.87      0.905      0.738\n",
      "                skis       5000        241      0.746       0.45      0.569      0.335\n",
      "           snowboard       5000         69      0.716      0.536      0.634      0.467\n",
      "         sports ball       5000        260      0.836      0.638      0.736      0.538\n",
      "                kite       5000        327       0.67      0.642      0.703      0.501\n",
      "        baseball bat       5000        145      0.881      0.614      0.749      0.461\n",
      "      baseball glove       5000        148      0.824      0.628      0.725      0.465\n",
      "          skateboard       5000        179      0.879      0.827      0.869       0.68\n",
      "           surfboard       5000        267       0.82      0.597      0.696      0.458\n",
      "       tennis racket       5000        225      0.904       0.84        0.9      0.624\n",
      "              bottle       5000       1013      0.707      0.561      0.644      0.459\n",
      "          wine glass       5000        341      0.736      0.545      0.657      0.437\n",
      "                 cup       5000        895      0.693      0.601      0.687      0.524\n",
      "                fork       5000        215      0.748      0.566      0.646      0.459\n",
      "               knife       5000        325      0.691      0.305      0.446      0.285\n",
      "               spoon       5000        253      0.607      0.372      0.435      0.285\n",
      "                bowl       5000        623      0.676      0.543      0.625      0.479\n",
      "              banana       5000        370      0.646      0.332      0.453      0.292\n",
      "               apple       5000        236      0.474       0.29      0.322      0.232\n",
      "            sandwich       5000        177      0.701      0.525      0.602      0.465\n",
      "              orange       5000        285      0.537      0.379      0.461      0.359\n",
      "            broccoli       5000        312      0.604      0.376      0.456      0.266\n",
      "              carrot       5000        365      0.456      0.359      0.345      0.237\n",
      "             hot dog       5000        125      0.705      0.464      0.601       0.47\n",
      "               pizza       5000        284       0.78        0.7      0.784      0.616\n",
      "               donut       5000        328      0.716      0.579      0.662      0.541\n",
      "                cake       5000        310      0.701      0.606      0.654      0.447\n",
      "               chair       5000       1771        0.7      0.499      0.592      0.397\n",
      "               couch       5000        261      0.752      0.571      0.679      0.521\n",
      "        potted plant       5000        342      0.637      0.541      0.585      0.356\n",
      "                 bed       5000        163      0.848      0.528      0.718      0.501\n",
      "        dining table       5000        695      0.675      0.386      0.478      0.324\n",
      "              toilet       5000        179      0.847      0.782       0.87      0.708\n",
      "                  tv       5000        288      0.855      0.778      0.847      0.671\n",
      "              laptop       5000        231      0.827      0.753       0.82       0.71\n",
      "               mouse       5000        106      0.825      0.811      0.848      0.692\n",
      "              remote       5000        283      0.739      0.591      0.655      0.441\n",
      "            keyboard       5000        153      0.758      0.641      0.749      0.576\n",
      "          cell phone       5000        262      0.719      0.584      0.671      0.474\n",
      "           microwave       5000         55      0.771      0.818      0.837      0.691\n",
      "                oven       5000        143      0.712      0.524      0.632      0.424\n",
      "             toaster       5000          9       0.72      0.667      0.631      0.439\n",
      "                sink       5000        225      0.733      0.573      0.688      0.478\n",
      "        refrigerator       5000        126      0.814      0.746      0.808      0.676\n",
      "                book       5000       1129      0.584      0.192      0.297      0.161\n",
      "               clock       5000        267      0.783      0.731      0.798      0.583\n",
      "                vase       5000        274       0.65       0.57      0.622      0.452\n",
      "            scissors       5000         36      0.821      0.417      0.508      0.423\n",
      "          teddy bear       5000        190      0.758      0.705      0.768      0.613\n",
      "          hair drier       5000         11      0.696     0.0909      0.136      0.122\n",
      "          toothbrush       5000         57      0.668      0.495      0.502      0.329\n",
      "Speed: 0.1ms pre-process, 10.5ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5l_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=18.68s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.56s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.654\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.511\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.603\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.556\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.386\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.628\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.723\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5x on COCO val2017\n",
    "!python val.py --weights yolov5l.pt --verbose --data coco.yaml --img 640 --batch-size 32 --conf-thres 0.05 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e31935",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/dogfound/neural_network/yolov5/data/coco.yaml, weights=['projects/COCO-street-finetune/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.05, iou_thres=0.45, task=val, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-62-g5f603a9 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients, 109.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco/val2017.cache' images and labels... 4952 found, \u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.678       0.55      0.635      0.464\n",
      "              person       5000      10777      0.746      0.765      0.819      0.607\n",
      "             bicycle       5000        314      0.776      0.436      0.611      0.403\n",
      "                 car       5000       1918      0.867      0.034      0.432      0.338\n",
      "          motorcycle       5000        367      0.841      0.577      0.733      0.499\n",
      "            airplane       5000        143       0.89      0.776      0.867       0.69\n",
      "                 bus       5000        283      0.916      0.651      0.813      0.684\n",
      "               train       5000        190      0.846      0.784      0.856      0.653\n",
      "               truck       5000        414      0.669      0.176       0.43      0.315\n",
      "                boat       5000        424      0.718      0.175       0.43      0.269\n",
      "       traffic light       5000        634      0.772      0.423      0.589      0.342\n",
      "        fire hydrant       5000        101      0.976      0.792      0.908      0.753\n",
      "           stop sign       5000         75      0.879      0.627      0.775      0.693\n",
      "       parking meter       5000         60      0.811      0.567      0.705      0.544\n",
      "               bench       5000        411      0.742      0.217      0.465      0.341\n",
      "                bird       5000        427      0.506      0.489      0.539      0.371\n",
      "                 cat       5000        202      0.647      0.827      0.833      0.673\n",
      "                 dog       5000        218      0.242       0.86      0.714      0.564\n",
      "               horse       5000        272      0.858      0.732      0.833      0.591\n",
      "               sheep       5000        354      0.752      0.686      0.776      0.574\n",
      "                 cow       5000        372      0.895      0.551      0.772      0.585\n",
      "            elephant       5000        252      0.871      0.778      0.851       0.67\n",
      "                bear       5000         71      0.948      0.773       0.88      0.692\n",
      "               zebra       5000        266      0.907      0.809      0.891      0.672\n",
      "             giraffe       5000        232      0.887      0.842      0.882      0.711\n",
      "            backpack       5000        371      0.448      0.307      0.344      0.201\n",
      "            umbrella       5000        407      0.701      0.518       0.65      0.456\n",
      "             handbag       5000        540        0.4      0.322      0.341      0.214\n",
      "                 tie       5000        252      0.582      0.619       0.64      0.421\n",
      "            suitcase       5000        299      0.779      0.468      0.646      0.439\n",
      "             frisbee       5000        115      0.764      0.791      0.815      0.612\n",
      "                skis       5000        241      0.629      0.409      0.516      0.323\n",
      "           snowboard       5000         69       0.64      0.464      0.575      0.436\n",
      "         sports ball       5000        260      0.859      0.596      0.748      0.544\n",
      "                kite       5000        327        0.7      0.521      0.673      0.481\n",
      "        baseball bat       5000        145      0.769      0.531      0.666      0.385\n",
      "      baseball glove       5000        148      0.682      0.628      0.709      0.424\n",
      "          skateboard       5000        179      0.878      0.782      0.855      0.598\n",
      "           surfboard       5000        267      0.691      0.494      0.606      0.418\n",
      "       tennis racket       5000        225      0.785      0.804      0.856      0.579\n",
      "              bottle       5000       1013      0.619      0.577      0.629      0.446\n",
      "          wine glass       5000        341      0.568      0.607       0.64       0.42\n",
      "                 cup       5000        895        0.6       0.61      0.653       0.49\n",
      "                fork       5000        215      0.505      0.586      0.587      0.429\n",
      "               knife       5000        325      0.476      0.345      0.416      0.257\n",
      "               spoon       5000        253      0.379      0.419      0.375      0.244\n",
      "                bowl       5000        623      0.612      0.506      0.549      0.414\n",
      "              banana       5000        370      0.513      0.358      0.439      0.287\n",
      "               apple       5000        236      0.427       0.25      0.302      0.215\n",
      "            sandwich       5000        177      0.495      0.508      0.511      0.383\n",
      "              orange       5000        285       0.43      0.404      0.422      0.323\n",
      "            broccoli       5000        312      0.495      0.404      0.415      0.243\n",
      "              carrot       5000        365      0.287      0.392      0.273      0.184\n",
      "             hot dog       5000        125      0.738      0.428        0.6      0.459\n",
      "               pizza       5000        284      0.656      0.701      0.716       0.56\n",
      "               donut       5000        328      0.691       0.49      0.615      0.498\n",
      "                cake       5000        310      0.645      0.526      0.605      0.407\n",
      "               chair       5000       1771      0.688      0.421      0.552      0.378\n",
      "               couch       5000        261      0.676      0.563      0.634       0.49\n",
      "        potted plant       5000        342      0.647      0.332      0.489      0.313\n",
      "                 bed       5000        163      0.627       0.62      0.647       0.44\n",
      "        dining table       5000        695      0.509      0.486      0.471      0.323\n",
      "              toilet       5000        179      0.804      0.709      0.772      0.621\n",
      "                  tv       5000        288      0.823      0.681      0.793      0.632\n",
      "              laptop       5000        231      0.735      0.736      0.802      0.671\n",
      "               mouse       5000        106      0.749       0.73      0.808       0.63\n",
      "              remote       5000        283      0.613      0.594      0.606       0.39\n",
      "            keyboard       5000        153      0.504       0.66      0.685      0.518\n",
      "          cell phone       5000        262      0.635       0.55      0.627      0.428\n",
      "           microwave       5000         55      0.757      0.782      0.812      0.665\n",
      "                oven       5000        143      0.564      0.538      0.574      0.383\n",
      "             toaster       5000          9      0.349      0.419      0.296      0.218\n",
      "                sink       5000        225      0.684      0.542      0.606      0.433\n",
      "        refrigerator       5000        126      0.787      0.722      0.799      0.625\n",
      "                book       5000       1129       0.45      0.244      0.296      0.149\n",
      "               clock       5000        267      0.802      0.708      0.788      0.572\n",
      "                vase       5000        274      0.636      0.542      0.597      0.429\n",
      "            scissors       5000         36      0.614      0.472      0.575      0.449\n",
      "          teddy bear       5000        190      0.693      0.714      0.752      0.587\n",
      "          hair drier       5000         11          1     0.0909      0.545      0.436\n",
      "          toothbrush       5000         57       0.46      0.439      0.475      0.297\n",
      "Speed: 0.1ms pre-process, 10.6ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp2/best_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=13.89s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.05s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.406\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.303\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.445\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.238\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.484\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.616\n",
      "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights projects/COCO-street-finetune/exp/weights/best.pt --verbose --data coco.yaml --img 640 --batch-size 32 --conf-thres 0.05 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0843772",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hivaze/CODE-W/PyCharm/lct_hack/yolov5/data/coco.yaml, weights=['projects/COCO-street-finetune-6/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.1, iou_thres=0.45, task=val, device=, single_cls=False, augment=False, verbose=True, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 üöÄ v6.0-13-gfc36064 torch 1.9.1+cu102 CUDA:0 (GeForce RTX 2060, 5934.5625MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients, 109.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco/val2017.cache' images and labels... 4952 found, \u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.684      0.553      0.642      0.477\n",
      "              person       5000      10777      0.753      0.761      0.815      0.609\n",
      "             bicycle       5000        314      0.754      0.459      0.634      0.434\n",
      "                 car       5000       1918      0.912     0.0652      0.484      0.393\n",
      "          motorcycle       5000        367      0.853      0.599      0.748       0.52\n",
      "            airplane       5000        143      0.886       0.79      0.864      0.694\n",
      "                 bus       5000        283      0.914      0.674      0.814      0.716\n",
      "               train       5000        190      0.848      0.795      0.854      0.654\n",
      "               truck       5000        414      0.673      0.215      0.441      0.344\n",
      "                boat       5000        424      0.703      0.201       0.45      0.297\n",
      "       traffic light       5000        634      0.749      0.412       0.58      0.346\n",
      "        fire hydrant       5000        101      0.988      0.812      0.904      0.758\n",
      "           stop sign       5000         75      0.868      0.613      0.774      0.701\n",
      "       parking meter       5000         60      0.777      0.567      0.715      0.579\n",
      "               bench       5000        411      0.716      0.238      0.494      0.371\n",
      "                bird       5000        427       0.56      0.475      0.558      0.391\n",
      "                 cat       5000        202      0.644      0.807      0.814      0.663\n",
      "                 dog       5000        218      0.247      0.862       0.71      0.572\n",
      "               horse       5000        272      0.833      0.739      0.828      0.666\n",
      "               sheep       5000        354      0.748      0.723      0.791      0.576\n",
      "                 cow       5000        372      0.847      0.609      0.753      0.589\n",
      "            elephant       5000        252      0.847      0.792      0.837      0.666\n",
      "                bear       5000         71      0.881      0.831      0.895      0.721\n",
      "               zebra       5000        266      0.904      0.812      0.882      0.701\n",
      "             giraffe       5000        232      0.889      0.858      0.881      0.713\n",
      "            backpack       5000        371      0.483       0.28      0.362      0.219\n",
      "            umbrella       5000        407      0.732      0.538      0.646      0.463\n",
      "             handbag       5000        540      0.502      0.283      0.389      0.251\n",
      "                 tie       5000        252      0.643      0.619      0.689      0.446\n",
      "            suitcase       5000        299      0.751      0.498      0.652      0.459\n",
      "             frisbee       5000        115      0.757      0.791      0.815      0.629\n",
      "                skis       5000        241      0.631      0.402      0.512      0.329\n",
      "           snowboard       5000         69      0.643      0.469      0.571      0.442\n",
      "         sports ball       5000        260      0.832      0.608      0.753      0.512\n",
      "                kite       5000        327      0.655      0.572      0.649      0.463\n",
      "        baseball bat       5000        145      0.784      0.545      0.695       0.39\n",
      "      baseball glove       5000        148      0.688      0.635      0.731       0.45\n",
      "          skateboard       5000        179      0.881      0.784       0.86      0.653\n",
      "           surfboard       5000        267      0.701      0.494      0.619      0.433\n",
      "       tennis racket       5000        225      0.816      0.796      0.854       0.57\n",
      "              bottle       5000       1013      0.626       0.58      0.635      0.444\n",
      "          wine glass       5000        341      0.597      0.607      0.649      0.436\n",
      "                 cup       5000        895      0.603      0.592      0.648      0.497\n",
      "                fork       5000        215      0.555      0.586      0.611       0.46\n",
      "               knife       5000        325      0.498      0.338      0.425      0.267\n",
      "               spoon       5000        253      0.407      0.411       0.39      0.255\n",
      "                bowl       5000        623      0.602      0.493      0.553      0.429\n",
      "              banana       5000        370      0.559      0.357      0.452      0.319\n",
      "               apple       5000        236      0.441      0.242      0.308      0.234\n",
      "            sandwich       5000        177      0.557       0.52      0.551       0.41\n",
      "              orange       5000        285      0.429      0.414      0.416      0.337\n",
      "            broccoli       5000        312      0.526      0.385      0.432      0.262\n",
      "              carrot       5000        365      0.274      0.357      0.259      0.172\n",
      "             hot dog       5000        125      0.798       0.44      0.636      0.505\n",
      "               pizza       5000        284      0.702      0.696      0.731      0.581\n",
      "               donut       5000        328      0.695      0.497      0.604      0.497\n",
      "                cake       5000        310      0.633      0.507      0.607      0.425\n",
      "               chair       5000       1771      0.708      0.426      0.572      0.401\n",
      "               couch       5000        261      0.654       0.59      0.644      0.498\n",
      "        potted plant       5000        342      0.626      0.327      0.482      0.302\n",
      "                 bed       5000        163      0.621      0.626      0.669       0.45\n",
      "        dining table       5000        695      0.504      0.488      0.483      0.338\n",
      "              toilet       5000        179      0.781      0.737      0.779      0.642\n",
      "                  tv       5000        288       0.82      0.674      0.785      0.636\n",
      "              laptop       5000        231       0.75      0.723      0.798      0.683\n",
      "               mouse       5000        106       0.79      0.743      0.813      0.636\n",
      "              remote       5000        283      0.633       0.58       0.61      0.408\n",
      "            keyboard       5000        153      0.559       0.66      0.682      0.533\n",
      "          cell phone       5000        262       0.67      0.531      0.631      0.443\n",
      "           microwave       5000         55      0.724      0.764      0.816       0.68\n",
      "                oven       5000        143      0.578      0.565      0.593      0.408\n",
      "             toaster       5000          9        0.3      0.333      0.238      0.172\n",
      "                sink       5000        225      0.713      0.524      0.611      0.423\n",
      "        refrigerator       5000        126      0.762       0.71      0.799      0.637\n",
      "                book       5000       1129      0.477      0.232      0.324      0.177\n",
      "               clock       5000        267      0.811        0.7      0.787      0.572\n",
      "                vase       5000        274      0.624      0.552      0.607      0.434\n",
      "            scissors       5000         36      0.541      0.444      0.564      0.462\n",
      "          teddy bear       5000        190      0.737      0.711       0.76      0.588\n",
      "          hair drier       5000         11          1     0.0909      0.545      0.436\n",
      "          toothbrush       5000         57      0.533      0.456      0.529      0.315\n",
      "Speed: 0.1ms pre-process, 10.4ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp4/best_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=11.12s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.71s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.355\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.391\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.184\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.210\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.456\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608\n",
      "Results saved to \u001b[1mruns/val/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5x on COCO val2017\n",
    "!python val.py --weights projects/COCO-street-finetune-6/exp/weights/best.pt --verbose --data coco.yaml --img 640 --batch-size 32 --conf-thres 0.1 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eacdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run YOLOv5x on COCO val2017\n",
    "!python val.py --weights projects/COCO-street-finetune-6/exp/weights/best.pt --verbose --data coco.yaml --img 640 --batch-size 32 --conf-thres 0.1 --iou-thres 0.45 --half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680c08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
